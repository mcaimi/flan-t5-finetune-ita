# PIPELINE DEFINITION
# Name: flan-t5-anon-ita-finetune-pipeline
# Description: Finetune a FLAN-T5 Seq2Seq model to anonymize PII in italian language
# Inputs:
#    author_name: str
#    cluster_domain: str
#    dataset_file_name: str
#    dataset_name: str
#    dataset_path: str
#    dataset_version: str
#    finetuned_model_path: str
#    hyperparameters: dict
#    model_allowed_patterns: str
#    model_name: str
#    model_path: str
#    model_version: str
#    pipeline_version: str
components:
  comp-convert-model:
    executorLabel: exec-convert-model
    inputDefinitions:
      artifacts:
        finetuned_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        checkpoint_dir:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        onnx_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-download-tar-from-s3:
    executorLabel: exec-download-tar-from-s3
    inputDefinitions:
      parameters:
        dataset_name:
          description: Name of the dataset (e.g. a folder under the dataset_bucket)
          parameterType: STRING
        dataset_version:
          description: Version of the dataset (also, a folder under the dataset_name
            path)
          parameterType: STRING
        file_name:
          description: '  Full key/path of the tar.gz file inside the bucket.'
          parameterType: STRING
    outputDefinitions:
      artifacts:
        output_tar:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-fetch-model:
    executorLabel: exec-fetch-model
    inputDefinitions:
      parameters:
        allowed_patterns:
          parameterType: STRING
        model_name:
          description: name of the repository that contains the checpoint on HF
          parameterType: STRING
        model_version:
          description: checkpoint version
          parameterType: STRING
    outputDefinitions:
      artifacts:
        original_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      parameters:
        dataset_dir:
          parameterType: STRING
        finetuned_model_dir:
          parameterType: STRING
        hyperparameters:
          parameterType: STRUCT
        original_model_dir:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        finetuned_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-unzip-data:
    executorLabel: exec-unzip-data
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        dataset_dir:
          parameterType: STRING
        model_dir:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset_properties:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        model_properties:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-convert-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - convert_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'optimum' 'transformers'\
          \ 'optimum[onnxruntime]' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef convert_model(\n    checkpoint_dir: str,\n    finetuned_model:\
          \ Input[Model],\n    onnx_model: Output[Model],\n):\n    from zipfile import\
          \ ZipFile\n    from pathlib import Path\n    import os\n\n    # import libraries\n\
          \    try:\n        import torch\n        from optimum.onnxruntime import\
          \ ORTModelForSeq2SeqLM\n        from transformers import AutoTokenizer\n\
          \    except ImportError as e:\n        print(f\"Exception during library\
          \ import {e}\")\n\n    # local dir\n    WORKDIR: str = f\"{checkpoint_dir}/workdir\"\
          \n    os.makedirs(WORKDIR, exist_ok=True)\n    # create output dir\n   \
          \ ONNX_DIR: str = f\"{checkpoint_dir}/onnx\"\n    os.makedirs(ONNX_DIR,\
          \ exist_ok=True)\n\n    # decompress finetuned model\n    with ZipFile(finetuned_model.path,\
          \ 'r') as ftuned:\n        ftuned.extractall(WORKDIR)\n\n    # load model\
          \ from local path via Optimum ONNX Optimizer\n    try:\n        tokenizer\
          \ = AutoTokenizer.from_pretrained(WORKDIR)\n        model = ORTModelForSeq2SeqLM.from_pretrained(\n\
          \            WORKDIR,\n            export=True\n        )\n    except Exception\
          \ as e:\n        print(f\"Exception during model export: {e}\")\n\n    #\
          \ save onnx to disk\n    try:\n        model.save_pretrained(ONNX_DIR)\n\
          \        tokenizer.save_pretrained(ONNX_DIR)\n    except Exception as e:\n\
          \        print(e)\n\n    # clean up\n    del model\n    del tokenizer\n\n\
          \    # save model to s3\n    onnx_model._set_path(onnx_model.path + \"-onnx.zip\"\
          )\n    onnx_path = Path(ONNX_DIR)\n\n    # zip & store\n    import zipfile\n\
          \    with zipfile.ZipFile(onnx_model.path, \"w\", zipfile.ZIP_DEFLATED)\
          \ as zip_file:\n        for entry in onnx_path.rglob(\"*\"):\n         \
          \   zip_file.write(entry, entry.relative_to(onnx_path))\n\n"
        image: python:3.11
        resources:
          cpuLimit: 8.0
          memoryLimit: 24.0
          resourceCpuLimit: '8'
          resourceMemoryLimit: 24G
    exec-download-tar-from-s3:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - download_tar_from_s3
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'boto3' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef download_tar_from_s3(\n    dataset_name: str,\n    dataset_version:\
          \ str,\n    file_name: str,\n    output_tar: Output[Dataset],          #\
          \ path where the tar will be written\n):\n    \"\"\"\n    Downloads a tar.gz\
          \ file from an S3 bucket to the component output.\n\n    Args:\n       \
          \ dataset_name: Name of the dataset (e.g. a folder under the dataset_bucket)\n\
          \        dataset_version: Version of the dataset (also, a folder under the\
          \ dataset_name path)\n        file_name:   Full key/path of the tar.gz file\
          \ inside the bucket.\n        output_tar:  Artifact where the downloaded\
          \ tar will be stored.\n    \"\"\"\n    import boto3\n    import os\n   \
          \ from botocore.exceptions import ClientError\n\n    # get parameters from\
          \ env\n    s3_endpoint: str = os.getenv(\"AWS_S3_ENDPOINT\")\n    bucket_name:\
          \ str = os.getenv(\"AWS_S3_BUCKET\")\n\n    # build object name to download\n\
          \    object_name: str = f\"{dataset_name}/{dataset_version}/{file_name}\"\
          \n\n    # connect to S3 storage\n    s3_client = boto3.client(\"s3\", endpoint_url=s3_endpoint)\n\
          \n    try:\n        s3_client.download_file(bucket_name, object_name, output_tar.path)\n\
          \        print(f\"Downloaded {object_name} from bucket {bucket_name} to\
          \ {output_tar.path}\")\n    except ClientError as e:\n        raise RuntimeError(\n\
          \            f\"Failed to download {object_name} from {bucket_name}: {e}\"\
          \n        ) from e\n\n"
        image: python:3.11
    exec-fetch-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - fetch_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'huggingface_hub'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef fetch_model(\n    model_name: str,\n    model_version: str,\n\
          \    allowed_patterns: str,\n    original_model: Output[Model],\n):\n  \
          \  \"\"\"\n    Downloads a model checkpoint from HuggingFace repository\n\
          \n    Args:\n        model_name: name of the repository that contains the\
          \ checpoint on HF\n        model_version: checkpoint version\n        original_model:\
          \  Artifact where the downloaded model checkpoint will be stored.\n    \"\
          \"\"\n    try:\n        import os\n        import zipfile\n        from\
          \ pathlib import Path\n        import huggingface_hub as hf\n    except\
          \ Exception as e:\n        raise e\n\n    HF_TOKEN: str = os.getenv(\"HF_TOKEN\"\
          )\n\n    # Download model checkpoint from HuggingFace repositories\n   \
          \ local_path: str = \"/\".join((\"/tmp/\", model_name))\n    os.makedirs(local_path,\
          \ exist_ok=True)\n\n    print(f\"Downloading model checkpoint: {model_name}\"\
          )\n    model_path = hf.snapshot_download(repo_id=model_name,\n         \
          \                           allow_patterns=allowed_patterns,\n         \
          \                           revision=model_version,\n                  \
          \                  token=HF_TOKEN,\n                                   \
          \ local_dir=local_path)\n\n    # save output dataset to S3\n    original_model._set_path(original_model.path\
          \ + \".zip\")\n    srcdir = Path(local_path)\n\n    with zipfile.ZipFile(original_model.path,\
          \ \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n        for entry in srcdir.rglob(\"\
          *\"):\n            zip_file.write(entry, entry.relative_to(srcdir))\n\n"
        image: python:3.11
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'torch' 'transformers'\
          \ 'datasets' 'accelerate' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(\n    dataset_dir: str,\n    original_model_dir:\
          \ str,\n    finetuned_model_dir: str,\n    hyperparameters: dict,\n    finetuned_model:\
          \ Output[Model]\n):\n    try:\n        import os\n        import random\n\
          \        import torch\n        import json\n        import uuid\n      \
          \  import zipfile\n        from pathlib import Path\n\n        from transformers\
          \ import (\n            AutoTokenizer, # tokenizer model \n            AutoModelForSeq2SeqLM,\
          \ # main seq2seq model\n            Seq2SeqTrainingArguments,\n        \
          \    Seq2SeqTrainer,\n            DataCollatorForSeq2Seq # dataset collator\n\
          \        )\n        from datasets import load_dataset, Dataset, DatasetDict\n\
          \    except ImportError as e:\n        print(f\"Import error: {e}\")\n\n\
          \    # create output dir\n    os.makedirs(finetuned_model_dir, exist_ok=True)\n\
          \n    # detect accelerator\n    def detect_accelerator() -> (str, torch.dtype):\n\
          \        # detect discrete accelerator\n        if torch.cuda.is_available():\n\
          \            accelerator = \"cuda\"\n            dtype = torch.float16\n\
          \        else:\n            accelerator = \"cpu\"\n            dtype = torch.float32\n\
          \n        return (accelerator, dtype)\n\n    # create a new dataset class\
          \ that holds training data information\n    class CustomPIIDataset(torch.utils.data.Dataset):\n\
          \        def __init__(self, dataset_path: str) -> None:\n            from\
          \ pathlib import Path\n\n            # path that contains json datasets\n\
          \            self.dataset_path: Path = Path(dataset_path)\n\n          \
          \  # find datafiles\n            self.datasets: list = [f for f in self.dataset_path.glob(\"\
          **/*.json\")]\n\n            # load dataset\n            self.dataset: list\
          \ = []\n            for fname in self.datasets:\n                print(f\"\
          Loading {fname}...\")\n                try:\n                    with open(fname,\
          \ \"r\") as json_dataset:\n                        self.dataset.extend(json.load(json_dataset))\n\
          \                except UnicodeDecodeError as ue:\n                    print(f\"\
          Error decoding {fname} due to {ue}: Skipping file.\")\n\n        def __len__(self)\
          \ -> int:\n            return len(self.dataset)\n\n        def __getitem__(self,\
          \ idx) -> dict:\n            # return datapoint\n            return self.dataset[idx]\n\
          \n    # preprocessing stuff\n    class DataPreprocessor():\n        def\
          \ __init__(self, tokenizer, max_length: int = 256):\n            self.tokenizer\
          \ = tokenizer\n            self.max_length = max_length\n\n        # preprocessing\
          \ function to prepare dataset for training\n        def data_preprocess(self,\
          \ examples):\n            \"\"\"\n            Convert PII anonymization\
          \ data to T5 text-to-text format\n            Input: \"anonymize: [original\
          \ text]\"\n            Output: \"[anonymized text]\"\n            \"\"\"\
          \n            inputs = []\n            targets = []\n\n            for original,\
          \ anonymized in zip(examples['original'], examples['anonymized']):\n   \
          \             # Create input with task prefix\n                input_text\
          \ = f\"anonymize: {original}\"\n                inputs.append(input_text)\n\
          \n                # Target is the anonymized text\n                targets.append(anonymized)\n\
          \n            # Tokenize inputs\n            model_inputs = self.tokenizer(\n\
          \                inputs,\n                max_length=self.max_length,  #\
          \ Increased for longer Italian sentences\n                truncation=True,\n\
          \                padding=False\n            )\n\n            # Tokenize\
          \ targets\n            labels = self.tokenizer(\n                targets,\n\
          \                max_length=self.max_length,\n                truncation=True,\n\
          \                padding=False\n            )\n\n            model_inputs[\"\
          labels\"] = labels[\"input_ids\"]\n            return model_inputs\n\n \
          \   # detect the presence of an accelerator available for this task\n  \
          \  device, dtype = detect_accelerator()\n\n    # parameters\n    EPOCHS:\
          \ int = hyperparameters.get(\"epochs\", 1)\n    LEARNING_RATE: float = float(hyperparameters.get(\"\
          learning_rate\", 1e-4))\n    HAS_GPU: bool = (device == \"cuda\")\n    MAX_LENGTH:\
          \ int = int(hyperparameters.get(\"max_length\", 256))\n    OPTIMIZER: str\
          \ = hyperparameters.get(\"optimizer\", \"AdamW\")\n    BATCH_SIZE: int =\
          \ int(hyperparameters.get(\"batch_size\", 4))\n    TRAIN_VAL_SPLIT: float\
          \ = float(hyperparameters.get(\"train_val_split\", 0.8))\n    RUN_NAME:\
          \ str = f\"flan-t5-it-finetune_{uuid.uuid4()}\"\n\n    # load dataset from\
          \ disk...\n    it_pii_dataset: CustomPIIDataset = CustomPIIDataset(dataset_dir)\n\
          \    print(f\"Dataset Loaded! -> Processed {len(it_pii_dataset)} datapoints\"\
          )\n\n    # prepare randomized splits\n    random.shuffle(it_pii_dataset.dataset)\n\
          \    train_val_split = int(len(it_pii_dataset) * float(TRAIN_VAL_SPLIT))\n\
          \    train_examples: list = it_pii_dataset[:train_val_split]\n    val_examples:\
          \ list = it_pii_dataset[train_val_split:]\n\n    # Create datasets\n   \
          \ train_data = {\n        \"original\": [ex.get(\"source\") for ex in train_examples],\n\
          \        \"anonymized\": [ex.get(\"target\") for ex in train_examples]\n\
          \    }\n\n    val_data = {\n        \"original\": [ex.get(\"source\") for\
          \ ex in val_examples],\n        \"anonymized\": [ex.get(\"target\") for\
          \ ex in val_examples]\n    }\n\n    # the complete rebuilt dataset. this\
          \ is used for training\n    dataset = DatasetDict({\n        \"train\":\
          \ Dataset.from_dict(train_data),\n        \"validation\": Dataset.from_dict(val_data)\n\
          \    })\n\n    # Display dataset information\n    print(f\"Training examples:\
          \ {len(dataset['train'])}\")\n    print(f\"Validation examples: {len(dataset['validation'])}\"\
          )\n\n    # Load tokenizer and model\n    tokenizer = AutoTokenizer.from_pretrained(original_model_dir)\n\
          \    model = AutoModelForSeq2SeqLM.from_pretrained(original_model_dir).to(device)\n\
          \n    print(f\"Model loaded: {original_model_dir} on {device}\")\n    print(f\"\
          Model parameters: {model.num_parameters()}\")\n\n    # Instantiate Preprocessor\n\
          \    dp: DataPreprocessor = DataPreprocessor(tokenizer=tokenizer, max_length=MAX_LENGTH)\n\
          \n    # Process datasets\n    print(\"Processing datasets...\")\n    tokenized_datasets\
          \ = dataset.map(\n        dp.data_preprocess,\n        batched=True,\n \
          \       remove_columns=dataset['train'].column_names\n    )\n\n    # print\
          \ out final dataset\n    print(\"Tokenized datasets:\")\n    print(tokenized_datasets)\n\
          \    print(\"\\nFirst tokenized example (input):\")\n    print(tokenizer.decode(tokenized_datasets['train'][0]['input_ids']))\n\
          \n    ## PREPARE TRAINING STEP ##\n\n    # setup training parameters\n \
          \   training_args = Seq2SeqTrainingArguments(\n        output_dir=finetuned_model_dir,\n\
          \        eval_strategy=\"epoch\",\n        learning_rate=LEARNING_RATE,\
          \  # Higher learning rate for smaller dataset\n        per_device_train_batch_size=BATCH_SIZE,\n\
          \        per_device_eval_batch_size=BATCH_SIZE,\n        num_train_epochs=EPOCHS,\
          \  # More epochs for small dataset\n        weight_decay=0.01,\n       \
          \ save_total_limit=2,\n        predict_with_generate=True,\n        fp16=HAS_GPU,\
          \  # Use mixed precision if GPU available\n        dataloader_pin_memory=HAS_GPU,\
          \ # only on GPU equipped systems. also silences warnings on MPS devices\
          \ (Apple)\n        logging_steps=10,\n        save_strategy=\"epoch\",\n\
          \        load_best_model_at_end=True,\n        metric_for_best_model=\"\
          eval_loss\",\n        greater_is_better=False,\n        push_to_hub=False,\n\
          \        report_to=\"none\", run_name=RUN_NAME,\n    )\n\n    # Data collator:\n\
          \    # - Build data batches\n    # - dynamic padding\n    data_collator\
          \ = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n\
          \        padding=True\n    )\n\n    # Initialize Trainer\n    trainer =\
          \ Seq2SeqTrainer(\n        model=model,\n        args=training_args,\n \
          \       train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"\
          validation\"],\n        processing_class=tokenizer,\n        data_collator=data_collator\n\
          \    )\n\n    print(\"Trainer initialized successfully!\")\n\n    ## TRAIN\
          \ MODEL!\n    print(\"Starting training...\")\n    train_result = trainer.train()\n\
          \n    # report information\n    print(\"\\nTraining completed!\")\n    print(f\"\
          Training loss: {train_result.training_loss:.4f}\")\n    print(f\"Training\
          \ time: {train_result.metrics['train_runtime']:.2f} seconds\")\n\n    ##\
          \ EVALUATE MODEL\n    print(\"Evaluating model on validation set...\")\n\
          \    eval_result = trainer.evaluate()\n\n    print(\"\\nEvaluation Results:\"\
          )\n    for key, value in eval_result.items():\n        print(f\"{key}: {value}\"\
          )\n\n    ## SAVE MODEL\n    model.save_pretrained(finetuned_model_dir)\n\
          \    tokenizer.save_pretrained(finetuned_model_dir)\n\n    # save finetuned\
          \ model to S3\n    finetuned_model._set_path(finetuned_model.path + \".zip\"\
          )\n    srcdir = Path(finetuned_model_dir)\n\n    with zipfile.ZipFile(finetuned_model.path,\
          \ \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n        for entry in srcdir.rglob(\"\
          *\"):\n            zip_file.write(entry, entry.relative_to(srcdir))\n\n"
        image: python:3.11
        resources:
          cpuLimit: 8.0
          memoryLimit: 24.0
          resourceCpuLimit: '8'
          resourceMemoryLimit: 24G
    exec-unzip-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - unzip_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.12.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef unzip_data(\n    model_dir: str,\n    dataset_dir: str,\n   \
          \ model: Input[Model],\n    model_properties: Output[Artifact],\n    dataset:\
          \ Input[Dataset],\n    dataset_properties: Output[Artifact]\n):\n    # import\
          \ zipfile lib\n    import json\n    import os\n    import tarfile\n    from\
          \ pathlib import PosixPath\n    from zipfile import ZipFile\n\n    # make\
          \ sure directory exists\n    os.makedirs(model_dir, exist_ok=True)\n   \
          \ os.makedirs(dataset_dir, exist_ok=True)\n\n    # 1. decompress model to\
          \ disk\n    with ZipFile(model.path, 'r') as compressed_model:\n       \
          \ compressed_model.extractall(model_dir)\n\n    # save model properties\n\
          \    m_props = {\n        \"model_filename\": model.path,\n        \"model_architecture\"\
          : \"Seq2Seq\",\n        \"model_type\": \"T5\",\n        \"framework\":\
          \ \"torch\",\n        \"flavor\": \"small\",\n    }\n\n    model_properties.path\
          \ += \".json\"\n    with open(model_properties.path, \"w\") as artifact_dump:\n\
          \        json.dump(m_props, artifact_dump)\n\n    # 2. decompress dataset\
          \ to disk\n    with tarfile.open(dataset.path) as compressed_dataset:\n\
          \        compressed_dataset.extractall(dataset_dir)\n\n    # save dataset\
          \ properties\n    d_props = {\n        \"dataset_filename\": model.path,\n\
          \        \"dataset_format\": \"json\",\n        \"framework\": \"torch\"\
          ,\n    }\n\n    dataset_properties.path += \".json\"\n    with open(dataset_properties.path,\
          \ \"w\") as artifact_dump:\n        json.dump(d_props, artifact_dump)\n\n"
        image: python:3.11
pipelineInfo:
  description: Finetune a FLAN-T5 Seq2Seq model to anonymize PII in italian language
  name: flan-t5-anon-ita-finetune-pipeline
root:
  dag:
    tasks:
      convert-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-convert-model
        dependentTasks:
        - train-model
        inputs:
          artifacts:
            finetuned_model:
              taskOutputArtifact:
                outputArtifactKey: finetuned_model
                producerTask: train-model
          parameters:
            checkpoint_dir:
              componentInputParameter: finetuned_model_path
        taskInfo:
          name: convert-model
      download-tar-from-s3:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-download-tar-from-s3
        inputs:
          parameters:
            dataset_name:
              componentInputParameter: dataset_name
            dataset_version:
              componentInputParameter: dataset_version
            file_name:
              componentInputParameter: dataset_file_name
        taskInfo:
          name: download-tar-from-s3
      fetch-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-fetch-model
        inputs:
          parameters:
            allowed_patterns:
              componentInputParameter: model_allowed_patterns
            model_name:
              componentInputParameter: model_name
            model_version:
              componentInputParameter: model_version
        taskInfo:
          name: fetch-model
      train-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-model
        dependentTasks:
        - unzip-data
        inputs:
          parameters:
            dataset_dir:
              componentInputParameter: dataset_path
            finetuned_model_dir:
              componentInputParameter: finetuned_model_path
            hyperparameters:
              componentInputParameter: hyperparameters
            original_model_dir:
              componentInputParameter: model_path
        taskInfo:
          name: train-model
      unzip-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-unzip-data
        dependentTasks:
        - download-tar-from-s3
        - fetch-model
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: output_tar
                producerTask: download-tar-from-s3
            model:
              taskOutputArtifact:
                outputArtifactKey: original_model
                producerTask: fetch-model
          parameters:
            dataset_dir:
              componentInputParameter: dataset_path
            model_dir:
              componentInputParameter: model_path
        taskInfo:
          name: unzip-data
  inputDefinitions:
    parameters:
      author_name:
        parameterType: STRING
      cluster_domain:
        parameterType: STRING
      dataset_file_name:
        parameterType: STRING
      dataset_name:
        parameterType: STRING
      dataset_path:
        parameterType: STRING
      dataset_version:
        parameterType: STRING
      finetuned_model_path:
        parameterType: STRING
      hyperparameters:
        parameterType: STRUCT
      model_allowed_patterns:
        parameterType: STRING
      model_name:
        parameterType: STRING
      model_path:
        parameterType: STRING
      model_version:
        parameterType: STRING
      pipeline_version:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.12.2
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-convert-model:
          pvcMount:
          - constant: training
            mountPath: /data
            pvcNameParameter:
              runtimeValue:
                constant: training
        exec-download-tar-from-s3:
          secretAsEnv:
          - keyToEnv:
            - envVar: AWS_S3_ENDPOINT
              secretKey: AWS_S3_ENDPOINT
            - envVar: AWS_ACCESS_KEY_ID
              secretKey: AWS_ACCESS_KEY_ID
            - envVar: AWS_SECRET_ACCESS_KEY
              secretKey: AWS_SECRET_ACCESS_KEY
            - envVar: AWS_S3_BUCKET
              secretKey: AWS_S3_BUCKET
            - envVar: AWS_DEFAULT_REGION
              secretKey: AWS_DEFAULT_REGION
            secretName: s3-datasets
            secretNameParameter:
              runtimeValue:
                constant: s3-datasets
        exec-fetch-model:
          secretAsEnv:
          - keyToEnv:
            - envVar: HF_TOKEN
              secretKey: HF_TOKEN
            secretName: huggingface-secret
            secretNameParameter:
              runtimeValue:
                constant: huggingface-secret
        exec-train-model:
          pvcMount:
          - constant: training
            mountPath: /data
            pvcNameParameter:
              runtimeValue:
                constant: training
        exec-unzip-data:
          pvcMount:
          - constant: training
            mountPath: /data
            pvcNameParameter:
              runtimeValue:
                constant: training
