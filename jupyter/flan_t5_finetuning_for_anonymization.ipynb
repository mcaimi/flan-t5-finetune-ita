{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FLAN-T5-Small Fine-tuning for PII Anonymization (Italian)\n",
        "\n",
        "This notebook demonstrates how to fine-tune the FLAN-T5-small model for anonymizing Personally Identifiable Information (PII) in Italian text.\n",
        "\n",
        "## Overview\n",
        "- Model: FLAN-T5-small (80M parameters)\n",
        "- Task: PII Anonymization\n",
        "- Language: Italian\n",
        "- Approach: Text-to-text format (input text with PII â†’ anonymized output with placeholders)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import numpy as np\n",
        "    import random\n",
        "    import dotenv\n",
        "    import json\n",
        "\n",
        "    from transformers import (\n",
        "        AutoTokenizer, # tokenizer model \n",
        "        AutoModelForSeq2SeqLM, # main seq2seq model\n",
        "        Seq2SeqTrainingArguments,\n",
        "        Seq2SeqTrainer,\n",
        "        DataCollatorForSeq2Seq # dataset collator\n",
        "    )\n",
        "\n",
        "    from libs.utility import detect_accelerator, downloadFromHuggingFace\n",
        "    from libs.parameters import Properties\n",
        "    from libs.dataset import CustomPIIDataset, DataPreprocessor\n",
        "    from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "    # metrics\n",
        "    import wandb\n",
        "except ImportError as e:\n",
        "    print(f\"Import error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Setup Environment\n",
        "\n",
        "Declare variables and global identifiers that are used throughout this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load dotenv\n",
        "config_env: dict = dotenv.dotenv_values(\"./localenv\")\n",
        "\n",
        "P_FILE: str = config_env.get(\"PARAMETER_FILE\", \"parameters.yaml\")\n",
        "M_REPO: str = config_env.get(\"MODEL_REPO_ID\", \"google/flan-t5-small\")\n",
        "DATASET_PATH: str = config_env.get(\"DATASET_FILE\", \"../dataset\")\n",
        "DATASET_SPLIT: float = 0.8 # 80% train, 20% validation\n",
        "OUTPUT_DIR: str = config_env.get(\"OUTPUT_DIR\", \"flan-finetuned-ita\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for the presence of an accelerator\n",
        "device, dtype = detect_accelerator()\n",
        "print(f\"Using device: {device}/{dtype} for training.\")\n",
        "\n",
        "# load parameters\n",
        "params: Properties = Properties(P_FILE)\n",
        "print(f\"Loaded HF: Cache Dir: {params.config_parameters.huggingface.cache_dir}\\nDownloading to {params.config_parameters.huggingface.local_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# login to Weights and Biases to save metrics\n",
        "try:\n",
        "    wandb.login(key=params.config_parameters.wandb.apikey)\n",
        "except Exception as e:\n",
        "    print(f\"Wandb login failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# download model from HF repository\n",
        "try:\n",
        "    model_name: str = downloadFromHuggingFace(M_REPO,\n",
        "                                            cache_dir=params.config_parameters.huggingface.cache_dir,\n",
        "                                            local_dir=params.config_parameters.huggingface.local_dir,\n",
        "                                            apitoken=params.config_parameters.huggingface.apitoken)\n",
        "except Exception as e:\n",
        "    print(f\"Caught exception: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create machinery to manage a synthetic Italian PII Dataset\n",
        "\n",
        "Since there's no standard Italian PII anonymization dataset, we'll create synthetic training data with various PII types.\n",
        "Example source and target expressions are loaded from a `json` file.\n",
        "\n",
        "The format of a single datapoint is:\n",
        "\n",
        "`{\n",
        "    \"source\": \"Example data with PII\",\n",
        "    \"target\": \"Example data with PII Masked Out\"\n",
        "}`\n",
        "\n",
        "Also, a custom DataSet class is created to manage this dataset during training\n",
        "Load datataset from disk and prepare randomized splits:\n",
        "- `train_examples`: 80% of the data, used to train the model.\n",
        "- `val_examples`: 20% of the data, used to validate the model.\n",
        "\n",
        "Variables contain a list of dictionaries, each dictionary contains the following keys:\n",
        "- \"source\": the text to be classified.\n",
        "- \"target\": the anonymized text with placeholders in place of actual PII"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load dataset using custom class\n",
        "it_pii_dataset: CustomPIIDataset = CustomPIIDataset(\"../dataset\")\n",
        "print(f\"Dataset Loaded! -> Processed {len(it_pii_dataset)} datapoints\")\n",
        "\n",
        "# prepare randomized splits\n",
        "random.shuffle(it_pii_dataset.dataset)\n",
        "train_val_split = int(len(it_pii_dataset) * DATASET_SPLIT)\n",
        "train_examples: list = it_pii_dataset[:train_val_split]\n",
        "val_examples: list = it_pii_dataset[train_val_split:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare dataset for training.\n",
        "A dataset is a dictionary with this format:\n",
        "\n",
        "- `train`: Dataset table containing all training data in order\n",
        "- `validation`: Dataset table containing all validation data in order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "train_data = {\n",
        "    \"original\": [ex.get(\"source\") for ex in train_examples],\n",
        "    \"anonymized\": [ex.get(\"target\") for ex in train_examples]\n",
        "}\n",
        "\n",
        "val_data = {\n",
        "    \"original\": [ex.get(\"source\") for ex in val_examples],\n",
        "    \"anonymized\": [ex.get(\"target\") for ex in val_examples]\n",
        "}\n",
        "\n",
        "# the complete rebuilt dataset. this is used for training\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_dict(train_data),\n",
        "    \"validation\": Dataset.from_dict(val_data)\n",
        "})\n",
        "\n",
        "# Display dataset information\n",
        "print(f\"Training examples: {len(dataset['train'])}\")\n",
        "print(f\"Validation examples: {len(dataset['validation'])}\")\n",
        "\n",
        "print(\"\\nExample from training set:\")\n",
        "print(f\"Original: {dataset['train'][0]['original']}\")\n",
        "print(f\"Anonymized: {dataset['train'][0]['anonymized']}\")\n",
        "\n",
        "print(\"\\nExample from validation set:\")\n",
        "print(f\"Original: {dataset['validation'][0]['original']}\")\n",
        "print(f\"Anonymized: {dataset['validation'][0]['anonymized']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PII Categories that the model will learn\n",
        "\n",
        "After a training phase, the model hopefully will learn how to replace the following PII types with placeholders:\n",
        "\n",
        "- **[NOME]**: Names of people\n",
        "- **[INDIRIZZO]**: Street addresses\n",
        "- **[TELEFONO]**: Phone numbers\n",
        "- **[EMAIL]**: Email addresses\n",
        "- **[CARTA_CREDITO]**: Credit card numbers\n",
        "- **[CODICE_FISCALE]**: Italian fiscal codes (tax IDs)\n",
        "- **[DATA_NASCITA]**: Dates of birth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show some examples of the task\n",
        "print(\"PII Anonymization Examples:\\n\")\n",
        "for i in range(min(5, len(dataset['train']))):\n",
        "    example = dataset['train'][i]\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"  Input:  {example['original']}\")\n",
        "    print(f\"  Output: {example['anonymized']}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Model and Tokenizer\n",
        "\n",
        "Load the model and tokenizer from the local repository. Model should be already present in the filesystem if you have trained it before.\n",
        "Download is managed at step 2 of this notebook\n",
        "\n",
        "- `model`: FLAN-T5-small from huggingface\n",
        "\n",
        "Upon loading, move the model to a GPU `device` if such hardware is detected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "print(f\"Model loaded: {model_name} on {device}\")\n",
        "print(f\"Model parameters: {model.num_parameters()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Preprocess Dataset\n",
        "\n",
        "Prepare data for Training and Validation steps.\n",
        "\n",
        "- `data_preprocess`: Tokenize the input and output tokens using the tokenizer\n",
        "\n",
        "Preprocessing is applied to the whole dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instantiate Preprocessor\n",
        "dp: DataPreprocessor = DataPreprocessor(tokenizer=tokenizer)\n",
        "\n",
        "# Process datasets\n",
        "print(\"Processing datasets...\")\n",
        "tokenized_datasets = dataset.map(\n",
        "    dp.data_preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=dataset['train'].column_names\n",
        ")\n",
        "\n",
        "# print out final dataset\n",
        "print(\"Tokenized datasets:\")\n",
        "print(tokenized_datasets)\n",
        "print(\"\\nFirst tokenized example (input):\")\n",
        "print(tokenizer.decode(tokenized_datasets['train'][0]['input_ids']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Setup Training Arguments\n",
        "\n",
        "Now we set up the training arguments, which are used to configure the training process.\n",
        "\n",
        "- `OUTPUT_DIR`: The directory where the model will be saved.\n",
        "- `LEARNING_RATE`: The learning rate used while computing gradient descent.\n",
        "- `EPOCHS`: The number of training epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# parameters\n",
        "import uuid\n",
        "EPOCHS: int = 20\n",
        "LEARNING_RATE: float = 3e-4\n",
        "HAS_GPU: bool = (device == \"cuda\")\n",
        "RUN_NAME: str = f\"flan-t5-it-finetune_{uuid.uuid4()}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize weights and biases project\n",
        "wandb.init(\n",
        "    project=params.config_parameters.wandb.project,\n",
        "    name=RUN_NAME\n",
        ")\n",
        "\n",
        "# setup training parameters\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,  # Higher learning rate for smaller dataset\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=EPOCHS,  # More epochs for small dataset\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True,\n",
        "    fp16=HAS_GPU,  # Use mixed precision if GPU available\n",
        "    dataloader_pin_memory=HAS_GPU, # only on GPU equipped systems. also silences warnings on MPS devices (Apple)\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"wandb\", run_name=RUN_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data collator:\n",
        "# - Build data batches\n",
        "# - dynamic padding\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train the Model\n",
        "\n",
        "Now train the model over the dataset. Measure Loss and report back"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train model\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# report information\n",
        "print(\"\\nTraining completed!\")\n",
        "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Evaluating model on validation set...\")\n",
        "eval_result = trainer.evaluate()\n",
        "\n",
        "print(\"\\nEvaluation Results:\")\n",
        "for key, value in eval_result.items():\n",
        "    print(f\"{key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save the Finetuned Model\n",
        "\n",
        "Finetuned model is ready for consumption. Save it to disk.\n",
        "\n",
        "- `OUTPUT_DIR`: directory where the model will be saved\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model and tokenizer\n",
        "model.save_pretrained(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {OUTPUT_DIR}\")\n",
        "print(\"\\nYou can load the model later with:\")\n",
        "print(f\"  tokenizer = AutoTokenizer.from_pretrained('{OUTPUT_DIR}')\")\n",
        "print(f\"  model = AutoModelForSeq2SeqLM.from_pretrained('{OUTPUT_DIR}')\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "flant5",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
