{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109bcf55",
   "metadata": {},
   "source": [
    "# Data Preparation and Exploration (Tokenizer)\n",
    "\n",
    "This notebook contains code to experiment with the T5 Tokenizer, to correctly transform input data (e.g. original text) into processed data (anonymized text)\n",
    "\n",
    "- Load the Tokenizer\n",
    "- Load Dataset\n",
    "- Use the tokenizer to calculate tokens and attention masks\n",
    "- Use the tokenizer to calculate tokens in target_mode for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e749bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import dotenv\n",
    "    import json\n",
    "\n",
    "    from transformers import (\n",
    "        AutoTokenizer, # tokenizer model \n",
    "    )\n",
    "\n",
    "    from libs.utility import detect_accelerator, downloadFromHuggingFace\n",
    "    from libs.parameters import Properties\n",
    "    from libs.dataset import CustomPIIDataset, DataPreprocessor\n",
    "    from datasets import load_dataset, Dataset, DatasetDict\n",
    "except ImportError as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33940dfd",
   "metadata": {},
   "source": [
    "## 1. Load environment variables and settings \n",
    "\n",
    "Configure the notebook for runtime with custom environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c1d57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dotenv\n",
    "config_env: dict = dotenv.dotenv_values(\"./localenv\")\n",
    "\n",
    "P_FILE: str = config_env.get(\"PARAMETER_FILE\", \"parameters.yaml\")\n",
    "M_REPO: str = config_env.get(\"MODEL_REPO_ID\", \"google/flan-t5-small\")\n",
    "DATASET_PATH: str = config_env.get(\"DATASET_FILE\", \"../dataset\")\n",
    "DATASET_SPLIT: float = 0.8 # 80% train, 20% validation\n",
    "OUTPUT_DIR: str = config_env.get(\"OUTPUT_DIR\", \"flan-finetuned-ita\")\n",
    "\n",
    "# load parameters\n",
    "params: Properties = Properties(P_FILE)\n",
    "print(f\"Loaded HF: Cache Dir: {params.config_parameters.huggingface.cache_dir}\\nDownloading to {params.config_parameters.huggingface.local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c8a8b",
   "metadata": {},
   "source": [
    "Download the model if not already present on disk. This uses the HuggingFace Library to interface with remote repositories\n",
    "\n",
    "- MODEL_ID: `google/flan-t5-small`\n",
    "- cache_dir: where on local disk the checkpoint will be cached during download\n",
    "- local_dir: local path where the HF library will download the checkpoint to\n",
    "- apitoken: token to authenticate and interact with huggingface API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model from HF repository\n",
    "try:\n",
    "    model_name: str = downloadFromHuggingFace(M_REPO,\n",
    "                                            cache_dir=params.config_parameters.huggingface.cache_dir,\n",
    "                                            local_dir=params.config_parameters.huggingface.local_dir,\n",
    "                                            apitoken=params.config_parameters.huggingface.apitoken)\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87b696",
   "metadata": {},
   "source": [
    "Load the dataset into a custom `DataSet` class:\n",
    "\n",
    "- Every item contains a dictionary with `source` and `target` text input data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset using custom class\n",
    "it_pii_dataset: CustomPIIDataset = CustomPIIDataset(\"../dataset\")\n",
    "print(f\"Dataset Loaded! -> Processed {len(it_pii_dataset)} datapoints\")\n",
    "\n",
    "# print one datapoint\n",
    "print(it_pii_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba52af",
   "metadata": {},
   "source": [
    "Load the tokeniser from the T5 checkpoint using HuggingFace Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f96e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf5b91",
   "metadata": {},
   "source": [
    "## Tokenizer setup\n",
    "\n",
    "The anonymization task is essentially a text-to-text transformation operation in which some tokens are translated into other tokens.\n",
    "\n",
    "We may have to expand the tokenizer vocabulary in order to use these new tokens.\n",
    "\n",
    "The new tokens are:\n",
    "\n",
    "- [NAME]\n",
    "- [ADDRESS]\n",
    "- [CITY]\n",
    "- [EMAIL]\n",
    "- [DATE]\n",
    "- [TAXCODE]\n",
    "- [PHONE]\n",
    "- [CREDITCARD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280c00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new tokens to the tokenizer vocabulary\n",
    "new_tokens = [\n",
    "    \"[NAME]\",\n",
    "    \"[ADDRESS]\",\n",
    "    \"[CITY]\",\n",
    "    \"[EMAIL]\",\n",
    "    \"[DATE]\",\n",
    "    \"[TAXCODE]\",\n",
    "    \"[PHONE]\",\n",
    "    \"[CREDITCARD]\",\n",
    "]\n",
    "\n",
    "# update tokenizer\n",
    "tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae042cd7",
   "metadata": {},
   "source": [
    "## 2. Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457a344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic test data. Use the same sentences used during test of the PyTorch model\n",
    "# Test with Italian examples containing PII\n",
    "test_sentences: dict = [\n",
    "    \"Il signor Alessandro Bianchi abita in Via Nazionale 45, Milano.\",\n",
    "    \"Per contattare Giulia Rossi chiamare il 339-8765432 o scrivere a giulia.rossi@email.it\",\n",
    "    \"Il paziente Marco Esposito, nato il 25/08/1982, codice fiscale SPSMRC82M25H501Z.\",\n",
    "    \"Pagamento con carta 5123-4567-8901-2345 intestata a Francesca Lombardi.\",\n",
    "    \"Contattare la dottoressa Elena Ricci al numero 02-12345678, ufficio in Corso Italia 88, Roma.\",\n",
    "]\n",
    "\n",
    "# same sentences, with PII anonymized\n",
    "anonymized_sentences: list = [\n",
    "    \"Il signor [NAME] abita in [ADDRESS], [CITY]\",\n",
    "    \"Per contattare [NAME] chiamare il [PHONE] o scrivere a [EMAIL]\",\n",
    "    \"Il paziente [NAME], nato il [DATE], codice fiscale [TAXCODE]\",\n",
    "    \"Pagamento con carta [CREDITCARD] intestata a [NAME]\",\n",
    "    \"Contattare la dottoressa [NAME] al numero [PHONE], ufficio in [ADDRESS], [CITY]\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369947d",
   "metadata": {},
   "source": [
    "now use the tokenizer to encode some example text sentences for further computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c621dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the tokenizer to calculate token IDs for input strings\n",
    "tokenized_input_sentences: list = []\n",
    "for input_string in test_sentences:\n",
    "    tokenized_input_sentences.append(tokenizer(input_string))\n",
    "\n",
    "# use the tokenizer to calculate token IDs for output strings\n",
    "tokenized_output_sentences: list = []\n",
    "for input_string in anonymized_sentences:\n",
    "    tokenized_output_sentences.append(tokenizer(input_string))\n",
    "\n",
    "# assert input length matches\n",
    "assert(len(tokenized_input_sentences) == len(tokenized_output_sentences))\n",
    "\n",
    "# show tokenization example\n",
    "dataset_len: int = len(tokenized_input_sentences)\n",
    "datapoint: int = int((dataset_len*random.random()) % dataset_len)\n",
    "\n",
    "# display encoded and decoded form\n",
    "print(\"INPUT\")\n",
    "print(f\"Token ID: {tokenized_input_sentences[datapoint].get('input_ids')}\")\n",
    "print(f\"Attention Mask: {tokenized_input_sentences[datapoint].get('attention_mask')}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(tokenized_input_sentences[datapoint].get('input_ids'))}\")\n",
    "\n",
    "# display encoded and decoded form\n",
    "print(\"OUTPUT\")\n",
    "print(f\"Token ID: {tokenized_output_sentences[datapoint].get('input_ids')}\")\n",
    "print(f\"Attention Mask: {tokenized_output_sentences[datapoint].get('attention_mask')}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(tokenized_output_sentences[datapoint].get('input_ids'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7493fb",
   "metadata": {},
   "source": [
    "instantiate a preprocessor instance to begin working on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e59ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor: DataPreprocessor = DataPreprocessor(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1752f8",
   "metadata": {},
   "source": [
    "now, prepare the data set for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "    \"original\": [ex for ex in test_sentences],\n",
    "    \"anonymized\": [ex for ex in anonymized_sentences]\n",
    "}\n",
    "\n",
    "# the complete rebuilt dataset. this is used for training\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(train_data),\n",
    "})\n",
    "\n",
    "# have a look inside the dataset\n",
    "print(dataset)\n",
    "from pprint import pprint\n",
    "\n",
    "for item in dataset.get('train'):\n",
    "    pprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenization and trasformation to the train dataset\n",
    "# INPUT DATAPOINT:\n",
    "#   contains features: ORIGINAL TEXT, ANONYMIZED TEXT\n",
    "# OUTPUT DATAPOINT (compatible with huggingface trainer):\n",
    "#   contains features: INPUT_IDS, ATTENTION MASK, LABELS\n",
    "consolidated_dataset = dataset.map(\n",
    "    preprocessor.data_preprocess, # function to tokenize and prepare the datapoint to be consumed by the Trainer,\n",
    "    batched=True, # process the data point\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "# explore dataset\n",
    "from pprint import pprint\n",
    "print(consolidated_dataset)\n",
    "\n",
    "for item in consolidated_dataset.get('train'):\n",
    "    pprint(tokenizer.decode(item.get('input_ids')))\n",
    "    pprint(tokenizer.decode(item.get('labels')))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
