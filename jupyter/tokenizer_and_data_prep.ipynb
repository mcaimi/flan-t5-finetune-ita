{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "109bcf55",
   "metadata": {},
   "source": [
    "# Data Preparation and Exploration (Tokenizer)\n",
    "\n",
    "This notebook contains code to experiment with the T5 Tokenizer, to correctly transform input data (e.g. original text) into processed data (anonymized text)\n",
    "\n",
    "- Load the Tokenizer\n",
    "- Load Dataset\n",
    "- Use the tokenizer to calculate tokens and attention masks\n",
    "- Use the tokenizer to calculate tokens in target_mode for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e749bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/.pyenv/versions/3.12.11/envs/flant5/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    import random\n",
    "    import dotenv\n",
    "    import json\n",
    "\n",
    "    from transformers import (\n",
    "        AutoTokenizer, # tokenizer model \n",
    "    )\n",
    "\n",
    "    from libs.utility import detect_accelerator, downloadFromHuggingFace\n",
    "    from libs.parameters import Properties\n",
    "    from libs.dataset import CustomPIIDataset, DataPreprocessor\n",
    "    from datasets import load_dataset, Dataset, DatasetDict\n",
    "except ImportError as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33940dfd",
   "metadata": {},
   "source": [
    "## 1. Load environment variables and settings \n",
    "\n",
    "Configure the notebook for runtime with custom environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c1d57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HF: Cache Dir: /home/marco/.cache/huggingface\n",
      "Downloading to /home/marco/.cache/huggingface/local_repo\n"
     ]
    }
   ],
   "source": [
    "# load dotenv\n",
    "config_env: dict = dotenv.dotenv_values(\"./localenv\")\n",
    "\n",
    "P_FILE: str = config_env.get(\"PARAMETER_FILE\", \"parameters.yaml\")\n",
    "M_REPO: str = config_env.get(\"MODEL_REPO_ID\", \"google/flan-t5-small\")\n",
    "DATASET_PATH: str = config_env.get(\"DATASET_FILE\", \"../dataset\")\n",
    "DATASET_SPLIT: float = 0.8 # 80% train, 20% validation\n",
    "OUTPUT_DIR: str = config_env.get(\"OUTPUT_DIR\", \"flan-finetuned-ita\")\n",
    "\n",
    "# load parameters\n",
    "params: Properties = Properties(P_FILE)\n",
    "print(f\"Loaded HF: Cache Dir: {params.config_parameters.huggingface.cache_dir}\\nDownloading to {params.config_parameters.huggingface.local_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c8a8b",
   "metadata": {},
   "source": [
    "Download the model if not already present on disk. This uses the HuggingFace Library to interface with remote repositories\n",
    "\n",
    "- MODEL_ID: `google/flan-t5-small`\n",
    "- cache_dir: where on local disk the checkpoint will be cached during download\n",
    "- local_dir: local path where the HF library will download the checkpoint to\n",
    "- apitoken: token to authenticate and interact with huggingface API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ba3ce4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model checkpoint: google/flan-t5-small to /home/marco/.cache/huggingface/local_repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 12 files: 100%|██████████| 12/12 [00:00<00:00, 1432.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded model checkpoint /mnt/data_volume/Marco/cache_dir/huggingface/local_repo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download model from HF repository\n",
    "try:\n",
    "    model_name: str = downloadFromHuggingFace(M_REPO,\n",
    "                                            cache_dir=params.config_parameters.huggingface.cache_dir,\n",
    "                                            local_dir=params.config_parameters.huggingface.local_dir,\n",
    "                                            apitoken=params.config_parameters.huggingface.apitoken)\n",
    "except Exception as e:\n",
    "    print(f\"Caught exception: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87b696",
   "metadata": {},
   "source": [
    "Load the dataset into a custom `DataSet` class:\n",
    "\n",
    "- Every item contains a dictionary with `source` and `target` text input data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba94e0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../dataset/ita_example.json...\n",
      "Loading ../dataset/ita_dataset/1/ita_dataset.json...\n",
      "Dataset Loaded! -> Processed 76 datapoints\n",
      "{'source': 'Mi chiamo Mario Rossi e vivo a Roma.', 'target': 'Mi chiamo [NOME] e vivo a Roma.'}\n"
     ]
    }
   ],
   "source": [
    "# load dataset using custom class\n",
    "it_pii_dataset: CustomPIIDataset = CustomPIIDataset(\"../dataset\")\n",
    "print(f\"Dataset Loaded! -> Processed {len(it_pii_dataset)} datapoints\")\n",
    "\n",
    "# print one datapoint\n",
    "print(it_pii_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba52af",
   "metadata": {},
   "source": [
    "Load the tokeniser from the T5 checkpoint using HuggingFace Transformers Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f96e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cf5b91",
   "metadata": {},
   "source": [
    "## Tokenizer setup\n",
    "\n",
    "The anonymization task is essentially a text-to-text transformation operation in which some tokens are translated into other tokens.\n",
    "\n",
    "We may have to expand the tokenizer vocabulary in order to use these new tokens.\n",
    "\n",
    "The new tokens are:\n",
    "\n",
    "- [NAME]\n",
    "- [ADDRESS]\n",
    "- [CITY]\n",
    "- [EMAIL]\n",
    "- [DATE]\n",
    "- [TAXCODE]\n",
    "- [PHONE]\n",
    "- [CREDITCARD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280c00c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add new tokens to the tokenizer vocabulary\n",
    "new_tokens = [\n",
    "    \"[NAME]\",\n",
    "    \"[ADDRESS]\",\n",
    "    \"[CITY]\",\n",
    "    \"[EMAIL]\",\n",
    "    \"[DATE]\",\n",
    "    \"[TAXCODE]\",\n",
    "    \"[PHONE]\",\n",
    "    \"[CREDITCARD]\",\n",
    "]\n",
    "\n",
    "# update tokenizer\n",
    "tokenizer.add_tokens(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae042cd7",
   "metadata": {},
   "source": [
    "## 2. Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457a344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic test data. Use the same sentences used during test of the PyTorch model\n",
    "# Test with Italian examples containing PII\n",
    "test_sentences: dict = [\n",
    "    \"Il signor Alessandro Bianchi abita in Via Nazionale 45, Milano.\",\n",
    "    \"Per contattare Giulia Rossi chiamare il 339-8765432 o scrivere a giulia.rossi@email.it\",\n",
    "    \"Il paziente Marco Esposito, nato il 25/08/1982, codice fiscale SPSMRC82M25H501Z.\",\n",
    "    \"Pagamento con carta 5123-4567-8901-2345 intestata a Francesca Lombardi.\",\n",
    "    \"Contattare la dottoressa Elena Ricci al numero 02-12345678, ufficio in Corso Italia 88, Roma.\",\n",
    "]\n",
    "\n",
    "# same sentences, with PII anonymized\n",
    "anonymized_sentences: list = [\n",
    "    \"Il signor [NAME] abita in [ADDRESS], [CITY]\",\n",
    "    \"Per contattare [NAME] chiamare il [PHONE] o scrivere a [EMAIL]\",\n",
    "    \"Il paziente [NAME], nato il [DATE], codice fiscale [TAXCODE]\",\n",
    "    \"Pagamento con carta [CREDITCARD] intestata a [NAME]\",\n",
    "    \"Contattare la dottoressa [NAME] al numero [PHONE], ufficio in [ADDRESS], [CITY]\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5369947d",
   "metadata": {},
   "source": [
    "now use the tokenizer to encode some example text sentences for further computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c621dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT\n",
      "Token ID: [276, 4711, 297, 32, 975, 6998, 9, 305, 14574, 4278, 4834, 25580, 2394, 9596, 519, 2128, 16, 1422, 11564, 3, 9, 1410, 7, 658, 301, 8038, 986, 23, 5, 1]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded text: Pagamento con carta 5123-4567-8901-2345 intestata a Francesca Lombardi.</s>\n",
      "OUTPUT\n",
      "Token ID: [276, 4711, 297, 32, 975, 6998, 9, 3, 33714, 16, 1422, 11564, 3, 9, 3, 33707, 1]\n",
      "Attention Mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Decoded text: Pagamento con carta [CREDITCARD] intestata a [NAME]</s>\n"
     ]
    }
   ],
   "source": [
    "# use the tokenizer to calculate token IDs for input strings\n",
    "tokenized_input_sentences: list = []\n",
    "for input_string in test_sentences:\n",
    "    tokenized_input_sentences.append(tokenizer(input_string))\n",
    "\n",
    "# use the tokenizer to calculate token IDs for output strings\n",
    "tokenized_output_sentences: list = []\n",
    "for input_string in anonymized_sentences:\n",
    "    tokenized_output_sentences.append(tokenizer(input_string))\n",
    "\n",
    "# assert input length matches\n",
    "assert(len(tokenized_input_sentences) == len(tokenized_output_sentences))\n",
    "\n",
    "# show tokenization example\n",
    "dataset_len: int = len(tokenized_input_sentences)\n",
    "datapoint: int = int((dataset_len*random.random()) % dataset_len)\n",
    "\n",
    "# display encoded and decoded form\n",
    "print(\"INPUT\")\n",
    "print(f\"Token ID: {tokenized_input_sentences[datapoint].get('input_ids')}\")\n",
    "print(f\"Attention Mask: {tokenized_input_sentences[datapoint].get('attention_mask')}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(tokenized_input_sentences[datapoint].get('input_ids'))}\")\n",
    "\n",
    "# display encoded and decoded form\n",
    "print(\"OUTPUT\")\n",
    "print(f\"Token ID: {tokenized_output_sentences[datapoint].get('input_ids')}\")\n",
    "print(f\"Attention Mask: {tokenized_output_sentences[datapoint].get('attention_mask')}\")\n",
    "print(f\"Decoded text: {tokenizer.decode(tokenized_output_sentences[datapoint].get('input_ids'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7493fb",
   "metadata": {},
   "source": [
    "instantiate a preprocessor instance to begin working on the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e40e59ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor: DataPreprocessor = DataPreprocessor(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1752f8",
   "metadata": {},
   "source": [
    "now, prepare the data set for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6ee514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['original', 'anonymized'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n",
      "{'anonymized': 'Il signor [NAME] abita in [ADDRESS], [CITY]',\n",
      " 'original': 'Il signor Alessandro Bianchi abita in Via Nazionale 45, Milano.'}\n",
      "{'anonymized': 'Per contattare [NAME] chiamare il [PHONE] o scrivere a [EMAIL]',\n",
      " 'original': 'Per contattare Giulia Rossi chiamare il 339-8765432 o scrivere a '\n",
      "             'giulia.rossi@email.it'}\n",
      "{'anonymized': 'Il paziente [NAME], nato il [DATE], codice fiscale [TAXCODE]',\n",
      " 'original': 'Il paziente Marco Esposito, nato il 25/08/1982, codice fiscale '\n",
      "             'SPSMRC82M25H501Z.'}\n",
      "{'anonymized': 'Pagamento con carta [CREDITCARD] intestata a [NAME]',\n",
      " 'original': 'Pagamento con carta 5123-4567-8901-2345 intestata a Francesca '\n",
      "             'Lombardi.'}\n",
      "{'anonymized': 'Contattare la dottoressa [NAME] al numero [PHONE], ufficio in '\n",
      "               '[ADDRESS], [CITY]',\n",
      " 'original': 'Contattare la dottoressa Elena Ricci al numero 02-12345678, '\n",
      "             'ufficio in Corso Italia 88, Roma.'}\n"
     ]
    }
   ],
   "source": [
    "train_data = {\n",
    "    \"original\": [ex for ex in test_sentences],\n",
    "    \"anonymized\": [ex for ex in anonymized_sentences]\n",
    "}\n",
    "\n",
    "# the complete rebuilt dataset. this is used for training\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(train_data),\n",
    "})\n",
    "\n",
    "# have a look inside the dataset\n",
    "print(dataset)\n",
    "from pprint import pprint\n",
    "\n",
    "for item in dataset.get('train'):\n",
    "    pprint(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa31056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/5 [00:00<?, ? examples/s]/home/marco/.pyenv/versions/3.12.11/envs/flant5/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4006: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 453.11 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n",
      "'anonymize: Il signor Alessandro Bianchi abita in Via Nazionale 45, Milano.</s>'\n",
      "'Il signor [NAME] abita in [ADDRESS] , [CITY]</s>'\n",
      "\n",
      "\n",
      "('anonymize: Per contattare Giulia Rossi chiamare il 339-8765432 o scrivere a '\n",
      " 'giulia.rossi@email.it</s>')\n",
      "'Per contattare [NAME] chiamare il [PHONE] o scrivere a [EMAIL]</s>'\n",
      "\n",
      "\n",
      "('anonymize: Il paziente Marco Esposito, nato il 25/08/1982, codice fiscale '\n",
      " 'SPSMRC82M25H501Z.</s>')\n",
      "'Il paziente [NAME] , nato il [DATE] , codice fiscale [TAXCODE]</s>'\n",
      "\n",
      "\n",
      "('anonymize: Pagamento con carta 5123-4567-8901-2345 intestata a Francesca '\n",
      " 'Lombardi.</s>')\n",
      "'Pagamento con carta [CREDITCARD] intestata a [NAME]</s>'\n",
      "\n",
      "\n",
      "('anonymize: Contattare la dottoressa Elena Ricci al numero 02-12345678, '\n",
      " 'ufficio in Corso Italia 88, Roma.</s>')\n",
      "('Contattare la dottoressa [NAME] al numero [PHONE] , ufficio in [ADDRESS] , '\n",
      " '[CITY]</s>')\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# apply tokenization and trasformation to the train dataset\n",
    "# INPUT DATAPOINT:\n",
    "#   contains features: ORIGINAL TEXT, ANONYMIZED TEXT\n",
    "# OUTPUT DATAPOINT (compatible with huggingface trainer):\n",
    "#   contains features: INPUT_IDS, ATTENTION MASK, LABELS\n",
    "consolidated_dataset = dataset.map(\n",
    "    preprocessor.data_preprocess, # function to tokenize and prepare the datapoint to be consumed by the Trainer,\n",
    "    batched=True, # process the data point\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")\n",
    "\n",
    "# explore dataset\n",
    "from pprint import pprint\n",
    "print(consolidated_dataset)\n",
    "\n",
    "for item in consolidated_dataset.get('train'):\n",
    "    pprint(tokenizer.decode(item.get('input_ids')))\n",
    "    pprint(tokenizer.decode(item.get('labels')))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flant5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
