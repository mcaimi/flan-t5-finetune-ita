{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dde74a4",
   "metadata": {},
   "source": [
    "# Convert Finetuned FLAN-T5 Model from PyTorch format to ONNX\n",
    "\n",
    "Export the finetuned checkpoint for production deployment by saving it in a common standard format: ONNX.\n",
    "\n",
    "ONNX is compatible with multiple serving runtimes and it is kind of an Intermediate Representation that can be run indipendently from the toolkit/framework that the original model has been written in .\n",
    "\n",
    "`ONNX` is the Acronym for `Open Neural Network Exchange`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "try:\n",
    "    import torch\n",
    "    import os\n",
    "    from dotenv import dotenv_values\n",
    "    from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "    from transformers import AutoTokenizer\n",
    "except ImportError as e:\n",
    "    print(f\"Exception during library import {e}\")\n",
    "\n",
    "# load dotenv\n",
    "config_env: dict = dotenv_values(\"localenv\")\n",
    "\n",
    "# load configuration parameters\n",
    "CONFIG_FILE: str = config_env.get(\"PARAMETER_FILE\", \"parameters.yaml\")\n",
    "OUTPUT_DIR: str = config_env.get(\"OUTPUT_DIR\", \"flan-finetuned-ita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02fd3c",
   "metadata": {},
   "source": [
    "## 1. Load & Convert Model via Optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output dir\n",
    "ONNX_DIR: str = OUTPUT_DIR + \"/onnx\"\n",
    "os.makedirs(ONNX_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5774da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from local path via Optimum ONNX Optimizer\n",
    "try:\n",
    "    onnx_network_model = ORTModelForSeq2SeqLM.from_pretrained(OUTPUT_DIR, export=True)\n",
    "    onnx_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "except Exception as e:\n",
    "    print(f\"Exception during model export: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24f121",
   "metadata": {},
   "source": [
    "## 2. Save The Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save onnx to disk\n",
    "try:\n",
    "    onnx_network_model.save_pretrained(ONNX_DIR)\n",
    "    onnx_tokenizer.save_pretrained(ONNX_DIR)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
