{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dde74a4",
   "metadata": {},
   "source": [
    "# Convert Finetuned FLAN-T5 Model from PyTorch format to ONNX\n",
    "\n",
    "Export the finetuned checkpoint for production deployment by saving it in a common standard format: ONNX.\n",
    "\n",
    "ONNX is compatible with multiple serving runtimes and it is kind of an Intermediate Representation that can be run indipendently from the toolkit/framework that the original model has been written in .\n",
    "\n",
    "`ONNX` is the Acronym for `Open Neural Network Exchange`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b4ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "try:\n",
    "    import torch\n",
    "    import os\n",
    "    from dotenv import dotenv_values\n",
    "    from optimum.onnxruntime import ORTModelForSeq2SeqLM\n",
    "    from transformers import AutoTokenizer\n",
    "except ImportError as e:\n",
    "    print(f\"Exception during library import {e}\")\n",
    "\n",
    "# load dotenv\n",
    "config_env: dict = dotenv_values(\"localenv\")\n",
    "\n",
    "# load configuration parameters\n",
    "CONFIG_FILE: str = config_env.get(\"PARAMETER_FILE\", \"parameters.yaml\")\n",
    "OUTPUT_DIR: str = config_env.get(\"OUTPUT_DIR\", \"flan-finetuned-ita\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02fd3c",
   "metadata": {},
   "source": [
    "## 1. Load & Convert Model via Optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create output dir\n",
    "ONNX_DIR: str = OUTPUT_DIR + \"/onnx\"\n",
    "os.makedirs(ONNX_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5774da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from local path via Optimum ONNX Optimizer\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "    model = ORTModelForSeq2SeqLM.from_pretrained(\n",
    "        OUTPUT_DIR,\n",
    "        export=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Exception during model export: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a24f121",
   "metadata": {},
   "source": [
    "## 2. Save The Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3135c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save onnx to disk\n",
    "try:\n",
    "    model.save_pretrained(ONNX_DIR)\n",
    "    tokenizer.save_pretrained(ONNX_DIR)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# clean up\n",
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef5887f",
   "metadata": {},
   "source": [
    "## 3. Test the Converted Model\n",
    "\n",
    "Try to do inference with the ONNX model. Make sure it still works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720b200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic test data. Use the same sentences used during test of the PyTorch model\n",
    "# Test with Italian examples containing PII\n",
    "test_sentences = [\n",
    "    \"Il signor Alessandro Bianchi abita in Via Nazionale 45, Milano.\",\n",
    "    \"Per contattare Giulia Rossi chiamare il 339-8765432 o scrivere a giulia.rossi@email.it\",\n",
    "    \"Il paziente Marco Esposito, nato il 25/08/1982, codice fiscale SPSMRC82M25H501Z.\",\n",
    "    \"Pagamento con carta 5123-4567-8901-2345 intestata a Francesca Lombardi.\",\n",
    "    \"Contattare la dottoressa Elena Ricci al numero 02-12345678, ufficio in Corso Italia 88, Roma.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the onnx model\n",
    "try:\n",
    "    onnx_model = ORTModelForSeq2SeqLM.from_pretrained(ONNX_DIR)\n",
    "    onnx_tokenizer = AutoTokenizer.from_pretrained(ONNX_DIR)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e546456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare inference pipeline\n",
    "from transformers import pipeline\n",
    "gen = pipeline(\n",
    "    \"text2text-generation\", # text2text-generation is the name of the task supported by Seq2Seq models such as T5\n",
    "    model     = onnx_model,\n",
    "    tokenizer = onnx_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce415b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test input\n",
    "for prompt in test_sentences:\n",
    "    result = gen(prompt, max_new_tokens=50)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5056fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "del onnx_model\n",
    "del onnx_tokenizer\n",
    "del gen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flant5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
